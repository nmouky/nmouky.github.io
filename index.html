<!DOCTYPE html>
<html>
<body class="white"><div id="MathJax_Message" style="display: none;"></div>



<div id="container">

    <div class="storey border-top"> 

    <div class="nav-left"> <!--class="column w405 left">-->

    <a href="http://andymiller.github.io" style="color:black; text-decoration:none;"><h1>Andrew Miller</h1></a>

    </div>
    <div class="nav-right"> <!-- class="column w548 right extra-pad"> -->

      <div id="nav">
        <a href="http://andymiller.github.io/#publications">Publications</a> <br>
        <a href="http://andymiller.github.io/#talks">Talks</a> <br>
        <!--<a href="http://andymiller.github.io/#projects">Projects</a> <br>-->
        
          <a href="http://andymiller.github.io/blog">Blog</a> <br>
        

        <div class="iconwrapper">
            <div class="icon">
            <a href="https://github.com/andymiller">
            <img src="http://andymiller.github.io/images/icons/GitHub-Mark-120px-plus.png" style="width:40px;height:40px;">
            </a>
            </div>
            <div class="icon">
            <a href="https://twitter.com/_amiller_">
            <img src="http://andymiller.github.io/images/icons/twitter-64.png" style="width:40px;height:40px;">
            </a>
            </div>
            <div class="icon">
            <input id="email-button" style="outline: none;" type="image" src="http://andymiller.github.io/images/icons/email-14-128.png" width="40px" height="40px" border="0">
            </div>
        </div>
        <div id="email-div" style="color: rgb(0, 0, 102); display: none;">
            acm _at_ seas.harvard.edu
        </div>

      </div> <!-- #nav -->

    </div> <!-- .column -->

    <div style="clear:both;"> </div>

</div> <!-- .storey (header) -->



    <div class="storey border-top">

      <div style="display:table;">
        <div class="headshot">
          <img src="images/acm2.jpg" style="width:100%; max-width:180px;">
          <!--style="width:200px;float:left;padding:10px; padding-right:30px; height:100%;">-->
        </div>

        <div class="standfirst">
          Research Scientist <br>

          <div style="font-size:18px">
          Apple
          <font size="-1">[<a href="docs/acm_cv.pdf">CV</a>]</font>
          </div>

          <p class="intro">
             I am a research scientist on the Health AI team at Apple, working
             on new machine learning and statistical methods for health
             applications.  Before that, I was a postdoctoral research scientist
             at the
             <a href="https://datascience.columbia.edu/">Data Science Institute</a>
             at <a href="https://www.columbia.edu/">Columbia University</a>
             where I worked on methods for statistical modeling and inference
             with
             <a href="http://stat.columbia.edu/~cunningham/">John Cunningham</a>
             and <a href="http://www.cs.columbia.edu/~blei/">Dave Blei</a>.
             Before Columbia, I completed my PhD in
             Computer Science at <a href="https://www.seas.harvard.edu/">Harvard University</a>,
             advised by <a href="http://www.cs.princeton.edu/~rpa/">Ryan Adams</a>,
             with a focus on probabilistic modeling and scalable inference methods.

             I like to be application-driven; my applied work ranges
             from problems in astronomy to health care to sports analytics.
             On applications in health care, I have worked
             with <a href="http://sph.berkeley.edu/ziad-obermeyer-md">Ziad Obermeyer</a>
             and <a href="https://www.chicagobooth.edu/faculty/directory/m/sendhil-mullainathan">Sendhil Mullainathan</a>.
             On applications in sports analytics, I worked closely with 
             <a href="http://www.lukebornn.com/">Luke Bornn</a>.
          </p>
        </div> <!-- .standfirst -->
      </div><!-- table container -->
    </div> <!-- story border-top -->

    <!-- PUBLICATIONS ROW -->
    <div class="storey border-top">
    <h2 id="publications"> Publications</h2>

    <div class="pub">
    <ul>

      <li>
        <img src="images/insulin-glucose-MLHC-2020.jpg" class="pub-image">
        <div class="pub-info">
          <h3>Learning Insulin-Glucose Dynamics in the Wild</h3>
          <p>
            Andrew C. Miller, Nicholas J. Foti, and Emily B. Fox <br>
            <a href="https://www.mlforhc.org/accepted-papers"><i>Machine Learning for Healthcare</i></a>, 2020<br>
            [<a class="showSingle" ,="" target="23">abstract</a>]
            [<a href="https://arxiv.org/abs/2008.02852">arxiv</a>]
          </p>
          <div id="div23" class="abstract">
              We develop a new model of insulin-glucose dynamics for forecasting
              blood glucose in type 1 diabetics. We augment an existing
              biomedical model by introducing time-varying dynamics driven by a
              machine learning sequence model. Our model maintains a
              physiologically plausible inductive bias and clinically
              interpretable parameters -- e.g., insulin sensitivity -- while
              inheriting the flexibility of modern pattern recognition
              algorithms. Critical to modeling success are the flexible, but
              structured representations of subject variability with a sequence
              model. In contrast, less constrained models like the LSTM fail to
              provide reliable or physiologically plausible forecasts. We
              conduct an extensive empirical study. We show that allowing
              biomedical model dynamics to vary in time improves forecasting at
              long time horizons, up to six hours, and produces forecasts
              consistent with the physiological effects of insulin and
              carbohydrates.
          </div>
        </div>
      </li>

      <li>
        <img src="images/vae-empirical-bayes.png" class="pub-image">
        <div class="pub-info">
          <h3>Comment: Variational Autoencoders as Empirical Bayes</h3>
          <p>
            Yixin Wang, Andrew C. Miller, and David M. Blei <br>
            <i>Statistical Science</i>, 2019<br>
            [<a class="showSingle" ,="" target="22">abstract</a>]
            [<a href="https://projecteuclid.org/euclid.ss/1563501638">link</a>]
            <!--<a href="https://projecteuclid-org.ezproxy.cul.columbia.edu/euclid.ss/1563501631">original</a>-->
            [<a href="http://statweb.stanford.edu/~ckirby/brad/papers/2017BayesOBayesEBayes.pdf">original article</a> | 
             <a href="https://projecteuclid.org/euclid.ss/1563501639">rejoinder</a>]
          </p>
          <div id="div22" class="abstract"> 
            In this comment, we discuss the connection between empirical Bayes
            and the variational autoencoder (VAE), a popular statistical
            inference framework in the machine learning community. We hope this
            connection motivates new algorithmic approaches for empirical
            Bayesians and gives new perspectives on VAEs for machine learners.
          </div>
        </div>
      </li>

      <li>
        <img src="images/zebrafish-small.png" class="pub-image">
        <div class="pub-info">
          <h3>Probabilistic Models of Larval Zebrafish Behavior: Structure on Many Scales</h3>
          <p>
             Robert Evan Johnson, Scott Linderman, Thomas Panier,
             Caroline Lei Wee, Erin Song, Kristian Joseph Herrera,
             Andrew Miller, Florian Engert <br>
            [<a class="showSingle" ,="" target="21">abstract</a>]
            [<a href="https://www.biorxiv.org/content/10.1101/672246v1">bioarxiv</a>]
          </p>
          <div id="div21" class="abstract">
            Nervous systems have evolved to combine environmental information
            with internal state to select and generate adaptive behavioral
            sequences. To better understand these computations and their
            implementation in neural circuits, natural behavior must be
            carefully measured and quantified. Here, we
            collect high spatial resolution video of single zebrafish larvae swimming in a naturalistic environment
            and develop models of their action selection across exploration and hunting. Zebrafish larvae swim in
            punctuated bouts separated by longer periods of rest called interbout intervals. We take advantage of this
            structure by categorizing bouts into discrete types and representing their behavior as labeled sequences
            of bout-types emitted over time. We then construct probabilistic models – specifically, marked renewal
            processes – to evaluate how bout-types and interbout intervals are selected by the fish as a function of its
            internal hunger state, behavioral history, and the locations and properties of nearby prey. Finally, we
            evaluate the models by their predictive likelihood and their ability to generate realistic trajectories of
            virtual fish swimming through simulated environments. Our simulations capture multiple timescales
            of structure in larval zebrafish behavior and expose many ways in which hunger state influences their
            action selection to promote food seeking during hunger and safety during satiety.
          </div>
        </div>
      </li>

      <li>
        <img src="images/drvae.png" class="pub-image">
        <div class="pub-info">
          <h3>Discriminative Regularization for Latent Variable Models with Applications to Electrocardiography</h3>
          <p>Andrew C. Miller, Ziad Obermeyer, John P. Cunningham, and Sendhil Mullainathan<br>
            <i>International Conference on Machine Learning (ICML)</i>, 2019 <br>
            [<a class="showSingle" ,="" target="20">abstract</a>]
            [<a href="http://proceedings.mlr.press/v97/miller19a/miller19a.pdf">pdf</a>]
          </p>
          <div id="div20" class="abstract">
            Generative models often use latent variables to represent
            structured variation in high-dimensional data, such as images
            and medical waveforms. However, these latent variables may ignore
            subtle, yet meaningful features in the data. Some features may
            predict an outcome of interest (e.g. heart attack) but account for
            only a small fraction of variation in the data. We propose a
            generative model training objective that uses a black-box
            discriminative model as a regularizer to learn representations
            that preserve this predictive variation. With these
            discriminatively regularized latent variable models,
            we visualize and measure variation in the data that influence a
            black-box predictive model, enabling an expert to better
            understand each prediction. With this technique, we study models
            that use electrocardiograms to predict outcomes of clinical
            interest. We measure our approach on synthetic and real data
            with statistical summaries and an experiment carried out
            by a physician.
          </div>
        </div>
      </li>

      <li>
        <img src="images/amia-2019.png" class="pub-image">
        <div class="pub-info">
          <h3>A Comparison of Patient History- and EKG-based Cardiac Risk Scores</h3>
          <p>Andrew C. Miller, Ziad Obermeyer, and Sendhil Mullainathan<br>
            <i>Proceedings of the AMIA Summit on Clinical Research Informatics (CRI)</i>, 2019 <br>
            [<a class="showSingle" ,="" target="19">abstract</a>]
            [<a href="docs/miller-AMIA-2019.pdf">pdf</a>]
          </p>
          <div id="div19" class="abstract">
            Patient-specific risk scores can be used to identify individuals
            at elevated risk for cardiovascular disease. Typically,
            risk scores are based on patient habits and history —
            age, sex, race, smoking behavior, and prior vital signs and diagnoses.
            We explore an alternative source of information, a patient's
            raw electrocardiogram recording, and develop a score of
            patient risk of certain outcomes. We compare models that
            predict disease onset following an emergency department visit,
            including stroke, atrial fibrillation, and other adverse
            cardiac events.  We show that a learned representation
            (e.g.~deep neural network) of raw EKG waveforms can improve
            prediction of cardiac abnormalities over traditional risk factors. 
            Further, we show that a simple predictor based on segmented
            heart beats performs as well or better than a complex
            convolutional network recently shown to reliably automate
            arrhythmia detection in EKG observations.
            We analyze a large cohort of emergency department patients
            and show evidence that EKG-derived risk scores can be more
            robust to patient heterogeneity.
          </div>
        </div>
      </li>

      <li>
        <img src="images/ml4hc-2018-stability.png" class="pub-image">
        <div class="pub-info">
          <h3>Measuring the Stability of EHR- and EKG-based Predictive Models</h3>
          <p>Andrew C. Miller, Ziad Obermeyer, and Sendhil Mullainathan<br>
            <i><a href="https://ml4health.github.io/2018/">Machine Learning for Health</a> (NeurIPS Workshop)</i>, 2018 <br>
            [<a class="showSingle" ,="" target="21">abstract</a>]
            [<a href="https://arxiv.org/abs/1812.00210">arxiv</a>]
          </p>
          <div id="div21" class="abstract">
            Databases of electronic health records (EHRs) are increasingly
            used to inform clinical decisions.
            Machine learning algorithms can find patterns in EHRs that
            are predictive of future adverse outcomes.
            However, statistical models may be built upon patterns of
            health-seeking behavior that vary across patient subpopulations,
            leading to poor predictive performance when training on one
            patient population and predicting on another. 
            This note proposes two tests to better measure and understand
            model generalization.
            We use these tests to compare models derived from two data
            sources: (i) historical medical records, and
            (ii) electrocardiogram (EKG) waveforms.
            In a predictive task, we show that EKG-based models can
            be more stable than EHR-based models across different
            patient populations.
          </div>
        </div>
      </li>

      <li>
        <img src="images/ml4hc-2018-dipole.png" class="pub-image">
        <div class="pub-info">
          <h3>A Probabilistic Model of Cardiac Physiology and Electrocardiograms</h3>
          <p>Andrew C. Miller, Ziad Obermeyer, David M. Blei, John P. Cunningham, and Sendhil Mullainathan<br>
            <i><a href="https://ml4health.github.io/2018/">Machine Learning for Health</a> (NeurIPS Workshop)</i>, 2018 <br>
            [<a class="showSingle" ,="" target="20">abstract</a>]
            [<a href="https://arxiv.org/abs/1812.00209">arxiv</a>]
          </p>
          <div id="div20" class="abstract">
              An electrocardiogram (EKG) is a common, non-invasive test that
              measures the electrical activity of a patient's heart.
              EKGs contain useful diagnostic information about patient
              health that may not be found other electronic health record (EHR) data. 
              As multi-dimensional waveforms, they could be modeled using
              generic machine learning tools, such as a linear factor model
              or a variational autoencoder. 
              We take a different approach: we specify a model that directly
              represents the underlying electrophysiology of the heart
              and the EKG measurement process. 
              The resulting generative model can solve a variety of
              downstream tasks that standard supervised or unsupervised
              learning approaches cannot handle. 
              We apply our model to two datasets, including a sample
              of emergency department patients with traditional EKG
              reports that have a lot of missing data. 
              We show that our model can more accurately reconstruct
              missing data (measured by test reconstruction error) than
              a standard baseline when there is significant missing data.
              More broadly, this physiological representation creates
              features of heart function that may be useful in a variety
              of settings, including prediction, causal analysis, and discovery.
          </div>
        </div>
      </li>

      <li>
        <img src="images/celeste-2018-thumb.png" class="pub-image">
        <div class="pub-info">
          <h3>Approximate Inference for Constructing Astronomical Catalogs from Images</h3>
          <p>Jeffrey Regier, Andrew C. Miller, David Schlegel, Ryan P. Adams, Jon D. McAuliffe, and Prabhat<br>
            <i>Annals of Applied Statistics (in press)</i>, 2019 <br>
            [<a class="showSingle" ,="" target="18">abstract</a>]
            [<a href="https://arxiv.org/abs/1803.00113">arxiv</a>]
          </p>
          <div id="div18" class="abstract">
            We present a new, fully generative model for constructing
            astronomical catalogs from optical telescope image sets.
            Each pixel intensity is treated as a Poisson random variable
            with a rate parameter that depends on the latent properties of
            stars and galaxies. These latent properties are themselves
            random, with scientific prior distributions constructed from
            large ancillary datasets. We compare two procedures for
            posterior inference: Markov chain Monte Carlo (MCMC) and
            variational inference (VI). MCMC excels at quantifying uncertainty
            while VI is 1000× faster. Both procedures outperform the
            current state-of-the-art method for measuring celestial bodies’
            colors, shapes, and morphologies. On a supercomputer, the VI
            procedure efficiently uses 665,000 CPU cores (1.3 million hardware
            threads) to construct an astronomical catalog from 50
            terabytes of images.
          </div>
        </div>
      </li>

      <li>
        <img src="images/savi.png" class="pub-image">
        <div class="pub-info">
          <h3>Semi-Amortized Variational Autoencoders</h3>
          <p>Yoon Kim, Sam Wiseman, Andrew C. Miller, David Sontag, Alexander M. Rush<br>
            <i>International Conference on Machine Learning (ICML)</i>, 2018 <br>
            [<a class="showSingle" ,="" target="17">abstract</a>]
            [<a href="https://arxiv.org/abs/1802.02550">arxiv</a>]
          </p>
          <div id="div17" class="abstract">
            Amortized variational inference (AVI) replaces instance-specific 
            local inference with a global inference network. While 
            AVI has enabled efficient training of deep generative models
            such as variational autoencoders (VAE), recent empirical work
            suggests that inference networks can produce suboptimal
            variational parameters. We propose a hybrid approach, to use
            AVI to initialize the variational parameters and run stochastic
            variational inference (SVI) to refine them. Crucially, the local
            SVI procedure is itself differentiable, so the inference network
            and generative model can be trained end-to-end with
            gradient-based optimization. This semi-amortized approach
            enables the use of rich generative models without experiencing
            the posterior-collapse phenomenon common in training VAEs for
            problems like text generation. Experiments show this approach
            outperforms strong autoregressive and variational baselines on
            standard text and image datasets.
          </div>
        </div>
      </li>

      <li>
        <img src="images/tre-fig.png" class="pub-image">
        <div class="pub-info">
          <h3>Taylor Residual Estimators via Automatic Differentiation</h3>
          <p>Andrew C. Miller, Nicholas J. Foti, Ryan P. Adams<br>
            <i><a href="http://approximateinference.org/">Advances in Approximate Bayesian Inference</a> (NeurIPS Workshop)</i>, 2017 <br>
            [<a class="showSingle" ,="" target="16">abstract</a>]
            [<a href="http://approximateinference.org/2017/accepted/MillerEtAl2017.pdf">pdf</a>] 
          </p>
          <div id="div16" class="abstract">
            We develop a method for reducing the variance of Monte Carlo
            estimators against distributions with known moments,
            termed Taylor residual Monte Carlo estimators (TREs).
            We analyze the variance of TREs, and derive conditions under which
            TREs outperform the original Monte Carlo estimators in terms 
            of estimator variance. Additionally, modern automatic
            differentiation tools can be leveraged to efficiently
            compute these new estimators. The utility of TREs is demonstrated
            on a Monte Carlo variational inference problem.
          </div>
        </div>
      </li>

      <li>
        <img src="images/ecg-fig.png" class="pub-image">
        <div class="pub-info">
          <h3>A Hierarchical Generative Model of Electrocardiogram Records</h3>
          <p>Andrew C. Miller, Sendhil Mullainathan, Ziad Obermeyer <br>
            <i><a href="https://ml4health.github.io/2017/">Machine Learning for Health</a> (NeurIPS Workshop)</i>, 2017 <br>
            [<a class="showSingle" ,="" target="15">abstract</a>]
            [<a href="docs/miller-ml4hc-2017.pdf">pdf</a>]
          </p>
          <div id="div15" class="abstract">
            We develop a probabilistic generative model of electrocardiogram (EKG) tracings.
            Our model describes multiple sources of variation in EKGs, including patient-
            specific cardiac cycle morphology and between-cycle variation that leads to quasi-
            periodicity. We use a deep generative network as a flexible model component to
            describe variation in beat-specific morphology. We apply our model to a set of 549
            EKG records, including over 4,600 unique beats, and show that it is able to discover
            interpretable information, such as patient similarity and meaningful physiological
            features (e.g., T wave inversion).
          </div>
        </div>
      </li>

      <li>
        <img src="images/reduced-var-reparam-grads.png" class="pub-image">
        <div class="pub-info">
          <h3>Reducing Reparameterization Gradient Variance</h3>
          <p>Andrew C. Miller, Nicholas J. Foti, Alexander D'Amour, and Ryan P. Adams <br>
            <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, 2017 <br>
            [<a class="showSingle" ,="" target="1">abstract</a>]
            [<a href="https://arxiv.org/abs/1705.07880">arxiv</a>] 
            [<a href="https://github.com/andymiller/ReducedVarianceReparamGradients">code</a>]
          </p>
          <div id="div1" class="abstract">
            Optimization with noisy gradients has become ubiquitous in statistics
            and machine learning. Reparameterization gradients, or gradient
            estimates computed via the “reparameterization trick,” represent a
            class of noisy gradients often used in Monte Carlo variational
            inference (MCVI). However, when these gradient estimators are too
            noisy, the optimization procedure can be slow or fail to converge.
            One way to reduce noise is to use more samples for the gradient
            estimate, but this can be computationally expensive. Instead,
            we view the noisy gradient as a random variable, and form an
            inexpensive approximation of the generating procedure for the
            gradient sample. This approximation has high correlation with the
            noisy gradient by construction, making it a useful control variate
            for variance reduction. We demonstrate our approach on
            non-conjugate multi-level hierarchical models and a Bayesian neural
            net where we observed gradient variance reductions of
            multiple orders of magnitude (20-2,000x).
          </div>
        </div>
      </li>

      <li>
        <img src="images/vboost-workshop.png" ,="" class="pub-image">
        <div class="pub-info">
          <h3>Variational Boosting: Iteratively Refining Posterior Approximations</h3>
          <p>
            Andrew C. Miller, Nicholas J. Foti, and Ryan P. Adams <br>
            <i>International Conference on Machine Learning (ICML)</i>, 2017 <br>
            <i>Early version in <a href="http://approximateinference.org/">
                 AABI 2016</a> (NeurIPS Workshop)</i> <br>
            [<a class="showSingle" ,="" target="3">abstract</a>]
            [<a href="https://arxiv.org/abs/1611.06585">arxiv</a>]
            [<a href="https://github.com/andymiller/vboost">code</a>]
          </p>
          <div id="div3" class="abstract">
           Abstract: We present a black-box variational inference (BBVI) method to approximate
           intractable posterior distributions with an increasingly rich approximating class.
           Using mixture distributions as the approximating class, we first describe how to
           apply the re-parameterization trick and existing BBVI methods to mixtures. We
           then describe a method, termed Variational Boosting, that iteratively refines an
           existing approximation by defining and solving a sequence of optimization problems,
           allowing the practitioner to trade computation time for increased accuracy.
          </div>
        </div>
      </li>

      <li>
        <img src="https://media.giphy.com/media/xUA7b821YzI7ZqUN56/giphy.gif" class="pub-image">
        <div class="pub-info">
          <h3>Possession Sketches: Mapping NBA Strategies</h3>
          <p>
            Andrew C. Miller and Luke Bornn <br>
            <i>MIT Sloan Sports Analytics Conference, 2017 </i> <br>
             <b>third place, <a href="http://www.sloansportsconference.com/activities/research-papers/">MIT SSAC Research Paper Competition</a></b> <br>
            [<a class="showSingle" ,="" target="2">abstract</a>]
            [<a href="docs/acm-possession-sketches-final.pdf">pdf</a>]
          </p>
          <div id="div2" class="abstract">
             We develop a method for automatically organizing and exploring
             possessions of basketball player-tracks by offensive structure.
             Our method centers around building a data-driven dictionary of 
             individual player actions, and then fitting a global hierarchical
             model to all possessions, yielding a concise summary, or
             <i>sketch</i> of the collective action taken by the offensive team
             in a basketball possession.
          </div>
        </div>
      </li>

      <li>
        <img src="images/dynamic-point-clouds.png" ,="" class="pub-image">
        <div class="pub-info">
          <h3>Learning a Similarity Measure for Dynamic Point Clouds</h3>
          <p>
            Andrew C. Miller and Luke Bornn <br>
            <i>in submission</i> <br>
            [<a class="showSingle" ,="" target="4">abstract</a>]
          </p>
          <div id="div4" class="abstract">
            Abstract: We develop a novel measure of similarity between two dynamic point clouds,
            where a dynamic point cloud is a collection of spatiotemporal
            trajectories representing multiple agents moving and interacting.
            Certain types of variation in trajectory data make this challenging;
            two dynamic point clouds may describe the same joint action, 
            but sub-actions may occur at different speeds, spatial locations, 
            or may be performed by different agents. As such, for the purposes of
            clustering and classification, known similarity measures fail. To
            solve this problem we construct a similarity measure in two parts.
            We first construct a novel distance metric between two sets of points
            (i.e. static point clouds). We then integrate this distance metric
            into dynamic time warping, yielding a similarity measure between
            dynamic point clouds. The resulting similarity measure is invariant
            to permutation of the agents and robust to spatiotemporal variation.
            Importantly, we describe how to differentiate through dynamic time
            warping in order to learn a similarity measure specific to an objective function.
            We use our method to describe the similarity of basketball sequences
            using player-tracking data from the National Basketball Association.
          </div>
        </div>
      </li>

      <li>
        <img src="images/rslds.png" class="pub-image">
        <div class="pub-info">
          <h3>Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems</h3>
          <p>
            Scott W. Linderman, Matthew J. Johnson, Andrew C. Miller, Ryan P. Adams, David M. Blei, and Liam Paninski <br>
            <!-- <i>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), 2017</i> -->
            <i>AISTATS</i>, 2017 <br>
            [<a class="showSingle" ,="" target="5">abstract</a>]
            [<a href="https://arxiv.org/abs/1610.08466">arxiv</a>]
            [<a href="http://proceedings.mlr.press/v54/linderman17a/linderman17a.pdf">aistats</a>]
          </p>
          <div id="div5" class="abstract">
            Abstract: Many natural systems, such as neurons firing in the brain or basketball teams
            traversing a court, give rise to time series data with complex, nonlinear dynamics.
            We can gain insight into these systems by decomposing the data into segments
            that are each explained by simpler dynamic units. Building on switching linear
            dynamical systems (SLDS), we present a new model class that not only discovers
            these dynamical units, but also explains how their switching behavior depends on
            observations or continuous latent states.
            These "recurrent" switching linear dynamical systems provide further insight
            by discovering the conditions under which each unit is deployed, something that traditional
            SLDS models fail to do. We leverage recent algorithmic advances in approximate 
            inference to make Bayesian inference in these models easy, fast, and scalable.
          </div>
        </div>
      </li>

      <li>
        <img src="images/quasar.png" class="pub-image">
        <div class="pub-info">
          <h3>A Gaussian Process Model of Quasar Spectral Energy Distributions</h3>
          <p>
            Andrew C. Miller, Albert Wu, Jeffrey Regier, Jon McAuliffe, 
            Dustin Lang, Mr Prabhat, David Schlegel, and Ryan P. Adams <br>
            <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, 2015 <br>
            [<a class="showSingle" ,="" target="6">abstract</a>]
            [<a href="https://papers.nips.cc/paper/5960-a-gaussian-process-model-of-quasar-spectral-energy-distributions.pdf">pdf</a>]
          </p>
          <div id="div6" class="abstract">
            Abstract: We propose a method for combining two sources of astronomical data, 
            spectroscopy and photometry, which carry information about sources of light 
            (e.g., stars, galaxies, and quasars) at extremely different spectral resolutions. 
            Our model treats the spectral energy distribution (SED) of the radiation 
            from a source as a latent variable, hierarchically generating both 
            photometric and spectroscopic observations.  
            We place a flexible, nonparametric prior over the SED of a light source that 
            admits a physically interpretable decomposition, and allows us to tractably perform inference.  
            We use our model to predict the distribution of the redshift of a quasar 
            from five-band (low spectral resolution) photometric data, the so called 
            "photo-z" problem. 
            Our method shows that tools from machine learning and Bayesian statistics 
            allow us to leverage multiple resolutions of information to make accurate
            predictions with well-characterized uncertainties. 
          </div>
        </div>
      </li>

      <li>
        <img src="images/flu.png" class="pub-image">
        <div class="pub-info">
          <h3>Advances in nowcasting influenza-like illness rates using  search query logs</h3>
          <p>
            Vasileios Lampos, Andrew C. Miller, Steve Crossan, and Christian Stefansen <br>
            <i>Scientific Reports</i>, 2015 <br>
            [<a class="showSingle" ,="" target="7">abstract</a>]
            [<a href="http://www.nature.com/srep/2015/150803/srep12760/full/srep12760.html">pdf</a>]
          </p>
          <div id="div7" class="abstract">
            Description: This paper presents an improvement on the Google Flu Trends model, 
            an epidemiological surveillance tool for measuring the current
            rate of influenza like illness (ILI) in the population.  
            These methods relate patterns in user search queries to historical 
            influenza estimates to obtain real-time ILI estimates.
            We develop a non-linear model 
            based on Gaussian processes and a family of autoregressive
            models.  We compare it to many already proposed methods, assessing
            predictive performance over five years of flu seasons, 2008-2013, 
            and show that it obtains state of the art predictive performance. 
          </div>
        </div>
      </li>

      <li>
        <img src="images/celeste.jpg" class="pub-image">
        <div class="pub-info">
          <h3>Celeste: Variational inference for a generative model of 
          astronomical images</h3>
          <p>
            Jeffrey Regier, Andrew C. Miller, Jon McAuliffe, 
            Ryan Adams, Matt Hoffman, Dustin Lang, David Schlegel, and Mr Prabhat <br>
            <!-- <i>Proceedings of The 32nd International Conference on Machine Learning, pp. 2095–2103, 2015 </i> -->
            <i>International Conference on Machine Learning (ICML)</i>, 2015 <br>
            [<a class="showSingle" ,="" target="8">abstract</a>]
            [<a href="http://jmlr.org/proceedings/papers/v37/regier15.html">icml</a>]
          </p>
          <div id="div8" class="abstract">
            We present a new, fully generative model of optical telescope image 
            sets, along with a variational procedure for inference. Each pixel 
            intensity is treated as a Poisson random variable, with a rate 
            parameter dependent on latent properties of stars and galaxies. 
            Key latent properties are themselves random, with scientific 
            prior distributions constructed from large ancillary data sets. 
            We check our approach on synthetic images. We also run it on 
            images from a major sky survey, where it exceeds the performance
            of the current state-of-the-art method for locating celestial 
            bodies and measuring their colors.
          </div>
        </div>
      </li>

      <li>
        <img src="images/defense.png" class="pub-image">
        <div class="pub-info">
          <h3>Characterizing the Spatial Structure of 
              Defensive Skill in Professional Basketball</h3>
          <p>
            Alexander Franks, Andrew Miller, Luke Bornn, and Kirk Goldsberry <br>
            <i>The Annals of Applied Statistics</i>, 2015 <br>
            [<a class="showSingle" ,="" target="9">abstract</a>]
            [<a href="http://projecteuclid.org/euclid.aoas/1430226086">AoAS</a>]
            [<a href="http://arxiv.org/abs/1405.0231">arxiv</a>]
          </p>
          <div id="div9" class="abstract">
           Description: We develop a spatial model to analyze the defensive ability of 
           professional basketball players.  We first define two preprocessing 
           steps to find a representation of players and posessions, and then we 
           define a parametric model with effects that correspond to 
           interpretable defensive ability.  
          </div>
        </div>
      </li>

      <li>
        <img src="images/counterpoints.png" class="pub-image">
        <div class="pub-info">
          <h3>Counterpoints: Advanced Defensive Metrics for NBA Basketball</h3>
          <p>Alexander Franks*, Andrew Miller*, Luke Bornn, and Kirk Goldsberry <br>
            <i>MIT Sloan Sports Analytics Conference</i>, 2015 <br>
            <b>best paper award, <a href="http://www.sloansportsconference.com/activities/research-papers/">MIT SSAC Research Paper Competition</a></b> <br>
            [<a class="showSingle" ,="" target="10">more</a>]
            [<a href="http://www.sloansportsconference.com/wp-content/uploads/2015/02/SSAC15-RP-Finalist-Counterpoints2.pdf">pdf</a>] 
            [<a href="http://www.sloansportsconference.com/?page_id=462">talk</a>]
          </p>
          <div id="div10" class="abstract">
            <b>press:</b>
            <ul>
              <li><a href="http://grantland.com/features/department-of-defense/">
                  Grantland: Department of Defense</a>
              </li><li><a href="http://fivethirtyeight.com/datalab/whats-happening-at-the-2015-mit-sloan-sports-analytics-conference/">
                   FiveThirtyEight Conference Summary</a>
              </li><li><a href="http://www.cjr.org/analysis/in_defense_of_defense.php">
                  Columbia Journalism Review: In Defense of Defense</a>
            </li></ul>
            Description: This paper develops new advanced defensive metrics for measuring
            the ability of professional basketball players, 
            derived from player tracking data. We use a <i>who's
            guarding whom model</i> to define a new suite of metrics 
            designed to measure how suppressive and disruptive players are on 
            average, and throughout the entire possession.
          </div>
        </div>
      </li>

      <li>
        <img src="images/factorized.png" class="pub-image">
        <div class="pub-info">
          <h3>Factorized Point Process Intensities: A Spatial Analysis of 
           Professional Basketball</h3>
          <p>
            Andrew Miller, Luke Bornn, Ryan Adams and Kirk Goldsberry <br>
            <!--<i>International Conference on Machine Learning (ICML)</i>, 2014 -->
            <i>International Conference on Machine Learning (ICML)</i>, 2014 <br>
            [<a class="showSingle" ,="" target="11">more</a>]
            [<a href="http://arxiv.org/abs/1401.0942">arxiv</a>]
          </p>
          <div id="div11" class="abstract">
            Description: We develop a dimensionality reduction method that can be applied to collections
            of point processes on a common space.  Using this representation, we analyze the shooting 
            habits of professional basketball players, create a new characterization of 
            offensive player types and model shooting efficiency.    
          </div>
        </div>
      </li>

      <li>
        <img src="images/multi_gpu.png" class="pub-image">
        <div class="pub-info">
          <h3>A Heterogeneous Framework for Large-Scale Dense 3-d Reconstruction from Aerial Imagery</h3>
          <p>
            Andrew Miller, Vishal Jain and Joseph L. Mundy <br>
            [<a class="showSingle" ,="" target="12">more</a>]
          </p><div id="div12" class="abstract">
            This paper presents a scalable system of multiple GPUs and CPUs to 
            reconstruct dense 3-D models.  This is a continuation Miller 2011 (which constructed 
            models of size ~ 1 billion voxels) that extends the system to models in the 50-100 billion voxel range. 
            Results are shown for building a 3-d model of an area of about 2 square kilometers 
            (&lt; 1 meter resolution) represented by 50 billion voxels over 4 GPUs in near real-time. <br><br>

            Demo rendering: <br>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/NSmJtlDzGDw" frameborder="0" allowfullscreen=""></iframe>
          </div>
        </div>
      </li>

      <li>
        <img src="images/cvpr_workshop.png" class="pub-image">
        <div class="pub-info">
          <h3>A Multi-sensor Fusion Framework in 3-D</h3>
          <p>Vishal Jain, Andrew Miller and Joseph L. Mundy <br>
            <i>2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</i> <br>
            [<a class="showSingle" ,="" target="13">more</a>]
            [<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=06595893">pdf</a>]
          </p>
          <div id="div13" class="abstract">
            This paper presents a system that fuses both optical and infrared imagery to build a 
            volumetric model.  We develop a technique to tightly register multiple volumetric models, 
            and show the benefits of the multi-modal datasource by developing classifiers to 
            label high level features of the landscape (road, sidewalk, pavement, buildings, etc.). 
          </div>
        </div>
      </li>

      <li>
        <img src="images/gpgpu1.png" class="pub-image">
        <div class="pub-info">
          <h3>Real-time rendering and dynamic updating of 3-d volumetric data</h3>
          <p>
            Andrew Miller, Vishal Jain and Joseph L. Mundy <br>
            <i>Proceedings of the Fourth Workshop on General Purpose Processing on Graphics Processing Units</i>, ASPLOS 2011 <br>
            [<a class="showSingle" ,="" target="14">more</a>]
            [<a href="http://www.ece.neu.edu/groups/nucar/GPGPU4/files/miller.pdf">pdf</a>]
          </p>
          <div id="div14" class="abstract">
            We develop and optimize a parallel ray tracing-inspired algorithm for both constructing and 
            rendering a high fidelity 3-d volumetric model from aerial imagery.  This paper goes over the engineering
            effort to gain an 800x speedup over serial implementations using a single gpu.
          </div>
        </div>
      </li>

    </ul> <!-- end publication list! -->
    </div> <!-- end div#pub -->

    </div> <!-- publications storey row -->


    <!-- talks section -->
    <div class="storey border-top">
    <h2 id="talks">Recent Talks</h2>
      <ul style="list-style: disc; padding-left:20px; font-size:16px; line-height:22px">
        <li>A Comparison of Patient History- and EKG-based Cardiac Risk Scores, AMIA Informatics Summit (Mar 2018)
        </li><li>Statistics, Machine Learning, and Computational Medicine, UCSB (Feb 2018)
        </li><li>Invited Speaker, Department of Computer Science, Rice University (Feb 2018)
        </li><li>Invited Speaker, Computer Science Department, UCLA (Feb 2018)
        </li><li><i>Taylor Residual Estimators via Automatic Differentiation</i>. Advances in Approximate Bayesian Inference, NeurIPS Workshop (Dec 2017)
        </li><li><i>Advances in Monte Carlo Variational Inference</i>. CS Colloquium, Viterbi School of Engineering, USC. (Nov 2017)
        </li><li><i>Possession Sketches: Mapping NBA Strategies</i>. MIT Sloan Sports Analytics Conference. (Mar 2017)
        </li><li><i>Stealing the Playbook: Structure discovery in NBA player-tracking data</i>. The Cascadia Symposium on Statistics in Sports. (Sept 2016)
        </li><li><i>Communication Panel</i>, NFL Football Performance and Technology Symposium, Indianapolis, IN. (Feb 2016)
        </li><li><i>Counterpoints: Advanced Defensive Metrics for NBA Basketball</i>. MIT Sloan Sports Analytics Conference. (Feb 2015)
        </li><li><i>Characterizing the Spatial Structure of Defensive Skill in Professional Basketball</i>. KDD Workshop on Large-Scale Sports Analytics. (Aug 2014)
        </li><li><i>A Spatiotemporal Analysis of Professional Basketball</i>. Joint Statistical Meetings, 2014. Session: Bayes and Big Data. (Aug 2014)
        </li><li><i>Factorized Point Process Intensities: A Spatial Analysis of Professional Basketball</i>. International Conference on Machine Learning, Beijing, China. (June 2014)
        </li><li><i>Quantifying Offensive Player Types in the NBA with Non-Negative Matrix Factorization</i>. New England Symposium on Statistics in Sports, Harvard University, Cambridge, MA. (Sept 2013)
      </li></ul>
    </div>
    <!-- talks div -->


    <!-- Software Project section -->
    <!--
    <div class="storey border-top">
    <h2 id="projects"> Projects</h2>
    <div class="projects">
    <ul style="list-style-type:none;">

      <li>
        <a href="https://github.com/jeff-regier/Celeste.jl">
          <img src="images/celestepy.png", width=120px>
        </a>
        <a href="https://github.com/jeff-regier/Celeste.jl"><h3>Celeste.jl</h3></a>
        <p>
        A julia library for astronomical source discovery and classification
        using approximate Bayesian inference (led by <a href="http://people.eecs.berkeley.edu/~jregier/">Jeff Regier</a>).
        </p>
      </li>

      <li>
        <a href="https://github.com/andymiller/pydtw">
        <img src="images/pydtw.png", width=120px>
        </a>
        <a href="https://github.com/andymiller/pydtw">
        <h3>pydtw</h3>
        </a>
        <p>Simple, lightweight dynamic time warping implementation (and 
           visualization) in numpy/python/cython.</p>
      </li>

      <li>
        <a href="https://github.com/mcleonard/sampyl">
        <img src="images/default.png", width=120px>
        </a>
        <a href="https://github.com/mcleonard/sampyl">
        <h3>Sampyl</h3>
        </a>
        <p>
        Sampyl is a package for sampling from probability distributions
        using MCMC methods. Similar to PyMC3 using theano to compute gradients,
        Sampyl uses autograd to compute gradients. However, you are free to 
        write your own gradient functions, autograd is not necessary. 
        This project was started as a way to use MCMC samplers by defining
        models purely with Python and numpy.
        </p>
      </li>
    </ul>
    </div>
    -->

</div> <!-- #container -->




<script>
$('#email-button').click(function() {
  $('#email-div').toggle('slow', function() {
    // Animation complete.
  });
});
</script>

<script>

$( "div.pub-info" ).click(function(event) {
    $(this).toggleClass("pub-full");
});
$( "div.pub-info" ).hover(function(event) {
    $(this).toggleClass("pub-full");
});
</script>






</body>
</html>

